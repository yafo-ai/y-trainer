{
  "model_path_to_load": "E:\\llm_model\\Qwen2.5-0.5B-Instruct\\",
  "use_lora": true,
  "data_type": "torch.bfloat16",
  "system_prompt": "You are a helpful assistant.",
  "use_deepspeed": false,
  "deepspeed_cfg_path": "",
  "gradit_checkpoing": true,
  "lora_path": "",
  "lora_rank": 16,
  "lora_alpha": 32.0,
  "lora_dropout": 0.2,
  "lora_target_modules": [
    "q_proj",
    "v_proj",
    "k_proj",
    "o_proj",
    "gate_proj",
    "up_proj",
    "down_proj"
  ],
  "epoch": 3,
  "lr": 5e-06,
  "batch_size": 1,
  "gradient_accumulation_steps": 1,
  "data_path": "./example_dataset/sft_example.json",
  "output_dir": "./LLM_models/TEST_SFT",
  "seed": 42,
  "use_tensorboard": false,
  "tensorboard_path": "",
  "use_process_bar": false,
  "training_type": "sft",
  "checkpoint_epoch": [
    1
  ],
  "local_rank": 0,
  "world_size": 1,
  "use_nlirg": true,
  "token_batch": 10,
  "distillition": false,
  "teacher_model_path": "",
  "coefficient_of_origin_loss": 0.5,
  "max_seq_len": 20520,
  "pack_length": -1,
  "rl_type": "agent"
}